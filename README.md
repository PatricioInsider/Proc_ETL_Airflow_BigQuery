<<<<<<< HEAD
# ðŸ“Š ETL Airflow: Ventas SRI de GCS a BigQuery

Este proyecto implementa un flujo ETL (Extract, Transform, Load) con **Apache Airflow**, para procesar archivos de ventas del SRI desde Google Cloud Storage (GCS), transformarlos con `pandas` y cargarlos en un Data Warehouse en **Google BigQuery**.

---

## ðŸ“ Estructura del Proyecto

etl-airflow/
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ airflow.cfg # ConfiguraciÃ³n personalizada de Airflow
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ descargar_csvs_gcs.py # DAG: descarga archivos CSV desde GCS a local
â”‚ â””â”€â”€ etl_ventas_gcs_bq.py # DAG: ETL desde CSV local a BigQuery
â”‚
â”œâ”€â”€ data/ # CSV de ventas desde 2017 hasta 2025
â”‚ â”œâ”€â”€ sri_ventas_2017.csv
â”‚ â”œâ”€â”€ ...
â”‚ â””â”€â”€ sri_ventas_2025.csv
â”‚
â”œâ”€â”€ docker-compose.yaml # OrquestaciÃ³n de Airflow con Docker
â”œâ”€â”€ requirements.txt # Paquetes necesarios para el entorno Airflow
â”œâ”€â”€ .env # Variables de entorno opcionales
â”œâ”€â”€ .gitignore # Archivos ignorados por Git
â”œâ”€â”€ claveCloude.json # ðŸ”’ Clave privada de GCP (NO incluida en el repo)

yaml
Copiar
Editar

---

## ðŸ“œ DescripciÃ³n de los DAGs

### `descargar_csvs_gcs.py`

- Conecta a un bucket de GCS mediante `gcsfs`.
- Descarga automÃ¡ticamente todos los archivos `.csv` y los guarda en `/opt/airflow/data`.
- Utiliza la credencial JSON localmente montada como volumen (`claveCloude.json`).

### `etl_ventas_gcs_bq.py`

- Lee los CSV desde la carpeta local.
- Realiza limpieza y transformaciÃ³n:
  - Crea dimensiones: `dim_provincia`, `dim_tiempo`, `dim_sector`.
  - Carga hechos: `hechos_ventas_compras`.
- Usa `pandas` y el cliente `google-cloud-bigquery`.

---

## ðŸ” Seguridad y acceso

- El archivo `claveCloude.json` **no se incluye en este repositorio** por seguridad.
  - EstÃ¡ listado en `.gitignore`.
  - Debe colocarse manualmente en la ruta `/opt/airflow/claveCloude.json` para que los DAGs funcionen.

- Los archivos `sri_ventas_*.csv` **sÃ­ se incluyen** porque son datos de acceso pÃºblico y sirven para pruebas y validaciones.  

---

## ðŸ”§ Requisitos

En el archivo `requirements.txt` se incluyen las dependencias principales:

```text
apache-airflow
pandas
gcsfs
google-cloud-bigquery
InstÃ¡lalas con:

bash
Copiar
Editar
pip install -r requirements.txt
ðŸ³ CÃ³mo levantar el entorno con Docker
bash
Copiar
Editar
docker-compose up -d
Accede a Airflow en: http://localhost:8080

Usuario/ContraseÃ±a por defecto:

pgsql
Copiar
Editar
admin / admin
â–¶ï¸ CÃ³mo ejecutar los DAGs
AsegÃºrate de tener la clave de servicio en la ruta correcta (claveCloude.json).

Ejecuta el DAG descargar_csvs_desde_gcs si necesitas descargar datos desde GCS.

Ejecuta el DAG etl_ventas_gcs_bigquery para transformar y cargar los datos a BigQuery.

ðŸ“Š Tablas generadas en BigQuery
Tabla	Tipo	DescripciÃ³n
dim_provincia	DimensiÃ³n	Provincia y cantÃ³n
dim_tiempo	DimensiÃ³n	AÃ±o, mes, trimestre
dim_sector	DimensiÃ³n	CÃ³digo de sector econÃ³mico + cantÃ³n
hechos_ventas_compras	Hechos	MÃ©tricas de ventas y compras del SRI

ðŸ”— Repositorio del Proyecto
Puedes consultar o clonar el repositorio desde:

https://github.com/tu-usuario/etl-airflow-sri
(Reemplaza este link con el real si es diferente)

=======
# Proc_ETL_Airflow_BigQuery
ConstrucciÃ³n de Procesos ETL con Apache Airflow y Google BigQuery
>>>>>>> 2aee45393d86053e092a0431bac6cb858a3a969c
